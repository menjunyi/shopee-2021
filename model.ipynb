import numpy as np
import pandas as pd
import sklearn

# k Folds-validation
NFOLDS = k # number of k folds validation
trains = []
valids = []
for fold_id in range(NFOLDS):
    trains.append(pd.read_csv(os.path.join(DATA_FOLDER, str(fold_id), 'train.csv')))
    valids.append(pd.read_csv(os.path.join(DATA_FOLDER, str(fold_id), 'valid.csv')))
#DATA_FOLDER is the folder name.

# Model
#Catboost
# CatBoost modeling
import optuna
from sklearn.metrics import matthews_corrcoef
from catboost import CatBoostClassifier

params = {
    ...
}
#input the name of params

cats = []
mcc_sum = 0.0 #If evaluation is according to mcc. (Could be replaced by other index)

for i in range(NFOLDS):
    class_weights = {0: 1.0, 1: 2} # case by case
    clf = CatBoostClassifier(class_weights=class_weights, **params)

    clf.fit(
        X_trains[i], y_trains[i],
        eval_set=(X_valids[i], y_valids[i]),
        use_best_model=True,
        plot=True,
    )
    
    mcc = matthews_corrcoef(y_valids[i], clf.predict(X_valids[i]))
    cats.append(clf)
    print(f'========== Results for fold {i} ==========')
    print('MCC:', mcc)
    mcc_sum += mcc
print('Avg MCC:', mcc_sum / NFOLDS)

#XGBoost & LGBM
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.estimators.random_forest import H2ORandomForestEstimator
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.estimators import H2OXGBoostEstimator
from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator
import time
from h2o.estimators import H2OXGBoostEstimator
from h2o.grid.grid_search import H2OGridSearch
from sklearn.metrics import log_loss

start = time.time()

xgb = H2OXGBoostEstimator(distribution='bernoulli', nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo').
# lgbbm model is differet in this part, other parts are the same
lgbm = H2OXGBoostEstimator(distribution='bernoulli', tree_method="hist", grow_policy="lossguide",
                           nfolds=10, keep_cross_validation_predictions=True, fold_assignment='Modulo',
                           ntrees=100, seed=11, score_tree_interval = 10,
                           stopping_rounds = 5, stopping_metric = "AUC", stopping_tolerance = 1e-4)
# distribution, nfolds can be changed.

xgb_params = {
    ...
}
#input the name of params

# Search criteria
search_criteria = {
    ...
}
#formula: 'name':'criteria'

# Make grid model
xgb_grid = H2OGridSearch(model=xgb,
                          grid_id='best_xgb_cmon',
                          hyper_params=xgb_params,
                          search_criteria=search_criteria)

# Train model
xgb_grid.train(x=X, y=Y, training_frame=h2o_train, validation_frame=h2o_val)

# Get best GLM
xgb_res = xgb_grid.get_grid(sort_by='auc', decreasing=True)
best_xgb = xgb_res.models[0]

#Random forest classifier
model = RandomForestClassifier(max_depth = 200, n_estimators = 300, n_jobs = -1, bootstrap = True, random_state = 123)
rf = model.fit(X_train, y_train)
y_pred = rf.predict(X_valid)
matthews_corrcoef(y_valid,y_pred)



#Prediction
def print_result(clfs, Xs):
    probs = np.zeros(shape=(Xs[0].shape[0], 2))

    for fold_id in range(len(clfs)):
        probs += clfs[fold_id].predict_proba(Xs[fold_id]) / NFOLDS
    preds = np.argmax(probs, axis=1)

    submission = pd.DataFrame({
        'row_id': test['row_id'],
        'open_flag': preds,
    })
    submission.to_csv('submission.csv', index=False)

print_result(cats, X_tests)#cats means training model


#feature importance
def imp_df(column_names, importances):
  df = pd.DataFrame({'variable': column_names,
                     'variable_importance': importances}) \
         .sort_values('variable_importance', ascending = False) \
         .reset_index(drop = True)
  return df
base_imp = imp_df(X_train.columns, rf.feature_importances_)
# Visualization of feature importance
plt.figure(figsize = (15,8))
fig = sns.barplot(x = 'variable_importance', y = 'variable', data = base_imp, orient = 'h', color = 'royalblue')
plt.title("Variable Importance using RFC", fontsize = 20)
plt.xlabel('Variable Importance', fontsize = 16)
plt.ylabel('')
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 14)
plt.show()